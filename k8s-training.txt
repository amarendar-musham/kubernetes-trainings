
======================================================
======================================================

K8S - Orchestration.

kube-scheduler default-scheduler -- algorithm (Go lang)



MASTER 
 - Typically consists of 
		1. kube-apiserver
		2. kube-scheduler
		3. kube-controller-manager (node/replication/job controller)
		4. etcd
		
 - Might container
		1. kube-proxy
		2. a network management utility
		
NODES
	1. kubelet
	2. kube-proxy
	3. cAdvisor
	4. Container runtime. 
	5. CNI - container network interface.
	
https://docs.microsoft.com/en-us/windows/wsl/install-win10
=windows subsystem for linux
	
minikube version
docker context ls
docker context use default
minikube start --driver=docker/hyperv # installs kube utitilies. downloads -> kubelet, kubectl, kubeadm

https://minikube.sigs.k8s.io/docs/start/
https://docs.docker.com/docker-for-windows/install/
https://docs.microsoft.com/en-us/windows/wsl/install-win10

kubectl get nodes --kubeconfig admin.conf ## run in node to connect master
/etc/kubernentes/admin.conf - in master

minkube stop
mv ~/.kube/config config_bkp ## which is from minikube
cp admin.conf ~/.kube/config # This is coming from master.
kubectl get nodes ## no need to specfy kubeconfig after the above copy.

minikube start ## append data to ~/.kube/config


kubectl config get-contexts
kubectl get nodes # shows only minikube as using its context
kubectl config use-context <kubernetes-admin@kubernetes>
kubectl config get-contexts #  * symbol shows on selected one.
kubectl get nodes # from the selected context. 



App+Dockerfile ----build---> image in Docker Engine --push--> DockerHUB. 

pod = storage + ip + security + {container1,2,..}

----------amarpod1.yaml
apiVersion: v1
kind: Pod
metadata:
 name: amarpod-123
spec:
 containers:
 - image: amarendarm/python:v1
   name: amarc1
   
kubectl apply -f amarpod1.yaml --dry-run=client
kubectl apply -f amarpod1.yaml
kubectl get pods/pos
kubectl get pods amarpod-123 -o wide
kubectl describe pod amarpod-123
kubectl delete pod amarpod-123

kubectl config get-contexts  AND docker context ls

both works based on the CRE(Container run-time environment.)

k8s - cluster setup - https://www.youtube.com/watch?v=TTzbQdu30YA&t=20s
https://github.com/redashu/k8s


masters count in a cluster? yes using LB, etcd DATA. 

-==========================================

minikube status
kubectl config get-contexts
docker context ls

---

kubectl get namespaces/ns # default 4, 
1. default
2. kube-node-lease # node specific pods /monitoring
3. kube-public
4. kube-system ## all k8s master/minion components

kubectl get po -n kube-system
kubectl create namespace amar-space
kubectl get pod --all-namespaces | grep -i amarpod-123 
kubectl  config  set-context  --current --namespace=amar-space

kubectl config get-contexts
kubectl get pod

-------------------------------------------
## SYSTEMCTL --from--> SYSTEMD---from-->kerner-boot. 
## systemctl won't work in container
## we need to use "httpd -DFOREGROUND" to start apache

git clone https://github.com/mdn/beginner-html-site-styled
git clone https://github.com/microsoft/project-html-website
git clone https://github.com/yenchiah/project-website-template


FROM oraclelinux:8.3
LABEL email="amar@oracle.com"
ENV deploy=hello
RUN dng install httpd -y
RUN mkdir /myapps /myapps/app1 /myapps/app2 /myapps/app3
COPY app1 /myapps/app1
COPY app2 /myapps/app2
COPY app3 /myapps/app3
COPY deploy.sh /myapps/
WORKDIR /myapps
RUN chmod +x deploy.sh
EXPOSE 80
ENTRYPOINT ["./deploy.sh"]

 
---deploy.sh----
#!/bin/bash

if [ "$deploy" == "app1" ]
then
    cp -rf /myapps/app1/* /var/www/html/
    httpd -DFOREGROUND
elif [ "$deploy" == "app2" ]
then
    cp -rf /myapps/app2/* /var/www/html/
    httpd -DFOREGROUND
elif [ "$deploy" == "app3" ]
then
    cp -rf /myapps/app3/* /var/www/html/
    httpd -DFOREGROUND
else
    echo "Hello, you need to check with Docker image builder team"
    httpd -DFOREGROUND
fi
---------------check if the image working.
docker run -dti --name x25 -p 425:80 -e deploy=app2 amaroracleweb:v1

http://localhost:425/ - working.

-----------------------push image to dockerhub. ##k8s can't recognize images inside the local machine. 

 docker images
 docker tag da820601f3f9 amarendarm/amaroracleweb:v111
 docker login -u amarendarm
 docker push amarendarm/amaroracleweb:v111

kubectl run  webapp1  --image=amarendarm/amaroracleweb:v111  --port=80  --dry-run=client -o yaml > amarapp1.yaml
kubectl run  webapp1  --image=amarendarm/amaroracleweb:v111  --port=80  --dry-run=client -o json > amarapp1.json

## creates a yaml/json file.  
## add env: deploy=app1 inside spec: column.
kubectl apply -f .\amarapp1.yaml
kubectl get po -o wide
kubectl describe po webapp1 -n amar-space

-----------------------------------------------------

K8S = CRE + CNI ## don't use bridge network of docker.

 - CRE from docker = container runtime engine ## creation containers. 
 - storage, security, network takes care by k8s. 
 - CNI = container network interface. 

CNI --> kubectl get po -n kube-system ## calico.
## CNI creates dynamic IP for the pods. which are not accessible outside.

laptop/local computer(kubectl + admin.conf) ---can-connect--> k8s master.

kubectl port-forward webapp1 9999:80 ## to access from local desktop http://localhost:9999

-------------------------------------------

---SERVICE---

service = pods Load balancer = staticIP:port 

etcd = No-SQL db = key:value

Services: 4
1. clusterIP
2. NodePort
3. LoadBalancer
4. ExternalName

service will be created on all nodes irrespective of pod is running on that node or not.

Nodeport = customer->minionIP:port -> serviceIP:port->bridge network -> pod (Pod doesn't require to be present on each minion/node)

customer -> LB name/ip -> minion{1,2,3}IP:port -> serviceIP:port -> bridge network -> pod

-------------------------------------------------------------

kubectl create service nodeport amarsvc1 --tcp 425:80 --dry-run=client -o yaml > svc1.yaml


selector = pod label (pod finder), get it from the pod yaml OR from the below command.

kubectl get po webapp1 --show-labels


kubectl apply -f .\svc1.yaml 
kubectl get svc/service
kubectl get svc -o wide ## show selector. (pod label)

Nodeport IP range: 30k - 32767

****
Access application  - minionIP:nodeport
http://3.231.204.110:31431/ 

redirected to svcIP:svcPORT(10.107.177.110:425) -> podIP:port (dynamiIP:80)



https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
kubectl apply -f url
kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
change clusterIP to nodePort



kubectl  get  secret  -n kubernetes-dashboard
kubectl  describe  secret kubernetes-dashboard-token-vddm9 -n kubernetes-dashboard

Apply client bindings. to give permissions to all name spaces.  ##kubectl apply -y clientbindings.yaml

-----------------------------------------------------------
----
Replication controller(deprecated), Replica set(almost deprecated), Deployment(latest)


kubectl create deployment amarapp2 --image amarendarm/amaroracleweb:v111 --dry-run=client -o yaml > amardep.yaml

echo --- >> amardep.yaml
kubectl  create  service  nodeport  amarsvc3  --tcp 1234:80 --dry-run=client -o yaml >> amardep.yaml


https://kubernetes.io/docs/reference/kubectl/cheatsheet/


kubectl get deploy -o wide
kubectl get rs -o wide
kubectl get po -o wide



k8s installation - https://www.youtube.com/watch?v=TTzbQdu30YA&t=20s

kubectl explain pod.spec ## like man command in linux.
kubectl api-resources 	|wc -l 


###To automatically match the app label of deployment to service. 

kubectl delete svc --all
kubectl get svc
kubectl get deploy
kubectl expose deploy amarapp2 --type NodePort --port 80 --name amarsvc1
kubectl get svc


##To do manual scaling on deployment.

kubectl scale deploy amarapp2 --replicas=3

-----------------------------------------------------------
docker build -t amarjavaweb https://github.com/amarendar-musham/javawebapp.git



phx.ocir.io/axmbtg8judkl/javaweb:v1
registry/namespace/repo:tag

docker login registry -u namespace/email
## password - auth-token.


kubectl create deploy amarapp111 --image=phx.ocir.io/axmbtg8judkl/javaweb:v1 --dry-run=client -o yaml  > javawebapp.yaml 
## add spec.containers.ports.containerPort: 8080 to  javawebapp.yaml


kubectl create secret
### docker-registry, generic, tls

kubectl  create  secret  docker-registry   amarsec2  --docker-server=phx.ocir.io  --docker-username=axmbtg8judkl/learntechbyme@gmail.com   --docker-password='<{Xog>]NQ[i0UrO;uWrz'


kubectl explain pod.spec | grep -i imagepull

# add spec.template.spec.imagePullSecrets.name = amarsec2 to javawebapp.yaml 

kubectl apply -f javawebapp.yaml

kubectl expose deploy amarapp111 --type NodePort --port 8080 --name amarsvc2

kubectl get svc # get the port and access in browser. 


---------------------------------------



kubectl expose deploy amardep --type LoadBalancer --port 80 --name amarsv2

## in OKE, the external IP loadbalancer gets created. But In k8s cluster, only internal IP loadbalancer gets created. 

## in OKE, master is not available physically.


kubectl delete all --all.


replace CMD with args
replace ENTRYPOINT with command.

kubectl  run  hellopod1  --image=alpine  --dry-run=client -o yaml  >alpine.yaml

##update spec.containers.command: ["sh","-c","ping 127.0.0.1"]
## to keep the container process up. 

kubectl replace -f alpine.yaml --force # delete existing container and re-creates. 

kubectl logs -f hellopoad1
kubectl exec -it hellopod1 -- sh

## update spec.volumes(-name: amarvol1, emptyDir: {}), takes space from scheduled minion node. 
## spec.containers.volumeMounts(-name: amarvol1, mountPath: /mnt/oracle) the above volume gets mounted here in container. 

## spec.containers.command: ["sh","-c","while true; do date>> /mnt/oracle/time.txt;sleep 5 ; done"]

kubectl replace -f .\alpine.yaml --force
kubectl exec -it hellopod1 -- sh 
cat /mnt/oracle/time.txt # exit

kubectl cp file1 hellopod1:/mnt/oracle

### read volume(logs) from a sidecar container.  
- image: nginx
    name: amarc1 ## sidecar container
    volumeMounts:
    - name: amarvol1
      mountPath: /usr/share/nginx/html
      readOnly: True
	  
kubectl replace -f .\alpine.yaml --force
kubectl exec -it hellopod1 -c amarc1 -- sh # pod name -c container name. 	

kubectl expose pod hellopod1 --type NodePort --port 80  --name amarsvc4

## Access minionIP:30kport. in browser. 

--------------------------------

kubectl create secret generic amardbsec  --from-literal sqlpass=Oracle099

env:
- name: MYSQL_ROOT_PASSWORD
  valueFrom:
    secretKeyRef:
     name: amardbsec
     key: sqlpass
	 
## create volume with NFS storage 
## add the same volume to mysql /var/lib/mysql 

kubectl apply -f .\microservice.yaml
kubectl get deploy,rs,po
kubectl logs -f amardb-566c54c7bc-5ds52

echo "---" > microservice.yaml
kubectl expose deploy amardb --type ClusterIP --port 3306 --dry-run=client -o yaml >> microservice.yaml

kubectl apply -f .\microservice.yaml
kubectl get svc # service with clusterIP, 3306. 

## add wordpress app to  microservice.yaml as another deployment "amarweb"

echo "---" >> microservice.yaml

kubectl expose deploy amarweb --type NodePort --port 80 --dry-run=client -o yaml >> microservice.yaml

kubectl apply -f .\microservice.yaml
kubectl get svc # add expose for web deployment.


kube-proxy - enforces network policies to secure the pod connectivy.  web to db. 
cAdvisor - when we don't want to give access to docker services for monitoring container. 

Ingress-controller - control LBs. 

REQUESTS --> DNS --> LB --> CLUSTER. 
one-ingress-controller one-load-balancer. It has a role to forward the requests to particular Services. 


Helm = installs/deploys k8s apis from repositories of yaml (like yum installs packages in repos)
